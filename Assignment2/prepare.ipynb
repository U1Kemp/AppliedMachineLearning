{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the data\n",
    "def load_data(filepath):\n",
    "    return pd.read_csv(filepath, sep='\\t', quoting=csv.QUOTE_NONE,names = ['label', 'message'])\n",
    "\n",
    "# set of english stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function for preprocessing messages\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Stopword removal\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Converting all text to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Removing empty strings\n",
    "    tokens = [token for token in tokens if token != '']\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# function for encoding ham as 0 and spam as 1\n",
    "def encode(text):\n",
    "    if text == 'spam':\n",
    "        return 1\n",
    "    elif text == 'ham':\n",
    "        return 0\n",
    "    \n",
    "# function to split into train and test data\n",
    "def split_data(df, seed = 42, train_size = 0.7, val_size = 0.15, test_size = 0.15):\n",
    "    train_df, val_test_df = train_test_split(df, test_size=1-train_size, random_state = seed)\n",
    "    val_df, test_df = train_test_split(val_test_df, test_size=test_size/(1-train_size), random_state = seed)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# function to store as a csv file\n",
    "def store_as_csv(df, name):\n",
    "    return df.to_csv(name, index=False)\n",
    "\n",
    "# function to print variable distribution\n",
    "def print_variable_distribution(data, file_name):\n",
    "    print(\"Number of 0s in \" + file_name + \" - \" + str(len(data) - data.spam.sum()))\n",
    "    print(\"Number of 1s in \" + file_name + \" - \" + str(data.spam.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sms data\n",
    "messages = load_data('data/raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_as_csv(messages,name='./data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\n",
      "\u001b[33mWhat's next?\u001b[39m\n",
      "\u001b[33m------------\u001b[39m\n",
      "- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n",
      "- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n",
      "- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc init --subdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding google drive as remote storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !dvc remote add --default myremote gdrive://1OONKIsu54eysQwijR4EaFTWV96e81NLB\n",
    "# !dvc remote modify myremote gdrive_acknowledge_abuse true\n",
    "# !dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .dvc/config\n",
    "# !git commit -m \"Adding Gdrive as Remote\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store raw_data.csv using dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'data/raw_data.csv' did not match any files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32mâ ‹\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/raw_data.csv |0.00 [00:00,     ?fi\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/home/utpalraj/coursework/AML/AppliedMachineLearning/Ass\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/raw_data.csv to cache     0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /home/utpalraj/coursework0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|1/1 [00:00,  8.40file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/raw_data.csv.dvc data/.gitignore\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0m[detached HEAD c06e4a6] Added raw_data.csv\n",
      " 3 files changed, 6 insertions(+)\n",
      " create mode 100644 Assignment2/.dvc/.gitignore\n",
      " create mode 100644 Assignment2/.dvc/config\n",
      " create mode 100644 Assignment2/.dvcignore\n"
     ]
    }
   ],
   "source": [
    "!git rm -r --cached 'data/raw_data.csv'\n",
    "!dvc add data/raw_data.csv \n",
    "!git commit -m \"Added raw_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and pipelines are up to date.                                              \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/utpalraj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/utpalraj/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/utpalraj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/utpalraj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['message'] = messages['message'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, jurong, point, ,, crazy, .., available, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, dun, say, early, hor, ..., u, c, already, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, i, n't, think, go, usf, ,, life, around,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>[freemsg, hey, darling, 's, 3, week, 's, word,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>[even, brother, like, speak, ., they, treat, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>[as, per, request, 'melle, melle, (, oru, minn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>[winner, !, !, as, valued, network, customer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>[had, mobile, 11, month, ?, u, r, entitled, up...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  [go, jurong, point, ,, crazy, .., available, b...\n",
       "1   ham           [ok, lar, ..., joking, wif, u, oni, ...]\n",
       "2  spam  [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
       "3   ham  [u, dun, say, early, hor, ..., u, c, already, ...\n",
       "4   ham  [nah, i, n't, think, go, usf, ,, life, around,...\n",
       "5  spam  [freemsg, hey, darling, 's, 3, week, 's, word,...\n",
       "6   ham  [even, brother, like, speak, ., they, treat, l...\n",
       "7   ham  [as, per, request, 'melle, melle, (, oru, minn...\n",
       "8  spam  [winner, !, !, as, valued, network, customer, ...\n",
       "9  spam  [had, mobile, 11, month, ?, u, r, entitled, up..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode ham as 0 and spam as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['label'] = messages['label'].apply(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into Train, Validation and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED1 = 2032\n",
    "SEED2 = 2001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using `SEED1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "train_df, val_df, test_df = split_data(messages, seed=SEED1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Train, Validation and Test Data as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training, validation, and testing sets to CSV files\n",
    "store_as_csv(train_df, name = './data/train.csv')\n",
    "store_as_csv(val_df, name = './data/validation.csv')\n",
    "store_as_csv(test_df, name = './data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and pipelines are up to date.                                              \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32mâ ‹\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/train.csv |0.00 [00:00,     ?file/\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/home/utpalraj/coursework/AML/AppliedMachineLearning/Ass\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/train.csv to cache        0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /home/utpalraj/coursework0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|1/1 [00:00, 11.20file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/.gitignore data/train.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32mâ ‹\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/validation.csv |0.00 [00:00,     ?\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/home/utpalraj/coursework/AML/AppliedMachineLearning/Ass\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/validation.csv to cache   0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /home/utpalraj/coursework0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|1/1 [00:00, 12.78file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/.gitignore data/validation.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc add \"./data/train.csv\"\n",
    "!dvc add \"./data/validation.csv\"\n",
    "!dvc add \"./data/test.csv\"\n",
    "!git add .\n",
    "!git commit -m \"Added train, validation and test data for SEED1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `SEED2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "train_df, val_df, test_df = split_data(messages, seed=SEED2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Train, Validation and Test Data as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training, validation, and testing sets to CSV files\n",
    "store_as_csv(train_df, name = './data/train.csv')\n",
    "store_as_csv(val_df, name = './data/validation.csv')\n",
    "store_as_csv(test_df, name = './data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train.csv.dvc:                                                             \n",
      "\tchanged outs:\n",
      "\t\tmodified:           data/train.csv\n",
      "data/validation.csv.dvc:\n",
      "\tchanged outs:\n",
      "\t\tmodified:           data/validation.csv\n",
      "data/test.csv.dvc:\n",
      "\tchanged outs:\n",
      "\t\tmodified:           data/test.csv\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l                                                                core\u001b[39m>\u001b[32mâ ‹\u001b[0m Checking graph\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/train.csv |0.00 [00:00,     ?file/\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/home/utpalraj/coursework/AML/AppliedMachineLearning/Ass\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/train.csv to cache        0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /home/utpalraj/coursework0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|1/1 [00:00, 28.23file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/train.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32mâ ‹\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/validation.csv |0.00 [00:00,     ?\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/home/utpalraj/coursework/AML/AppliedMachineLearning/Ass\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/validation.csv to cache   0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /home/utpalraj/coursework0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|1/1 [00:00, 38.93file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/validation.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32mâ ‹\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/test.csv |0.00 [00:00,     ?file/s\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/home/utpalraj/coursework/AML/AppliedMachineLearning/Ass\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/test.csv to cache         0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /home/utpalraj/coursework0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|1/1 [00:00, 21.80file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/test.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0m[detached HEAD 007735c] Added train, validation and test data for SEED2\n",
      " 4 files changed, 93 insertions(+), 23 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!dvc add \"./data/train.csv\"\n",
    "!dvc add \"./data/validation.csv\"\n",
    "!dvc add \"./data/test.csv\"\n",
    "!git add .\n",
    "!git commit -m \"Added train, validation and test data for SEED2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Variable Distribution of the First Version of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m007735c\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD\u001b[m\u001b[33m)\u001b[m Added train, validation and test data for SEED2\n",
      "\u001b[33mbb117d1\u001b[m Added train, validation and test data for SEED1\n",
      "\u001b[33m3373ed2\u001b[m Added raw_data.csv\n",
      "\u001b[33m6769486\u001b[m Added raw_data.csv\n",
      "\u001b[33mc636675\u001b[m Added raw_data.csv\n",
      "\u001b[33m2a43917\u001b[m Added raw_data.csv\n",
      "\u001b[33m0d7987c\u001b[m Added raw_data.csv\n",
      "\u001b[33m5ae244a\u001b[m Added raw_data.csv\n",
      "\u001b[33mb6d5f0c\u001b[m Added train, validation and test data for SEED2\n",
      "\u001b[33m24ba264\u001b[m Added train, validation and test data for SEED1\n",
      "\u001b[33ma89499a\u001b[m Added train, validation and test data for SEED1\n",
      "\u001b[33m412d939\u001b[m Added raw_data.csv\n",
      "\u001b[33med4422c\u001b[m Added raw_data.csv\n",
      "\u001b[33m74cfbb1\u001b[m Added raw_data.csv\n",
      "\u001b[33m07450eb\u001b[m Added raw_data.csv\n",
      "\u001b[33mbd49797\u001b[m Added raw_data.csv\n",
      "\u001b[33m271529a\u001b[m Added raw_data.csv\n",
      "\u001b[33maf419cc\u001b[m Added raw_data.csv\n",
      "\u001b[33m57052c5\u001b[m Added raw_data.csv\n",
      "\u001b[33m993a810\u001b[m Adding Gdrive as Remote\n",
      "\u001b[33m3a04280\u001b[m Added raw_data.csv\n",
      "\u001b[33m4f460cb\u001b[m Added raw_data.csv\n",
      "\u001b[33md860e38\u001b[m Added raw_data.csv\n",
      "\u001b[33m7d44a4e\u001b[m Added raw_data.csv\n",
      "\u001b[33m4b1db74\u001b[m Added raw_data.csv\n",
      "\u001b[33m856daa6\u001b[m Adding Gdrive as Remote\n",
      "\u001b[33m4e5e5e6\u001b[m\u001b[33m (\u001b[m\u001b[1;31morigin/main\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m Minor correction\n",
      "\u001b[33m4277718\u001b[m Corrected some minor errors\n",
      "\u001b[33ma60f4a4\u001b[m Minor Change to train.ipynb\n",
      "\u001b[33m57f6a3c\u001b[m updated notebooks\n",
      "\u001b[33m497550a\u001b[m updated notebooks\n",
      "\u001b[33m8c900f7\u001b[m Updated Notebooks\n",
      "\u001b[33mebfba18\u001b[m updated\n",
      "\u001b[33m4d1d301\u001b[m updated train.ipynb\n",
      "\u001b[33m4969443\u001b[m Updated Notebooks\n",
      "\u001b[33ma48d8d4\u001b[m Updated prepare.ipynb\n",
      "\u001b[33m52bbc8e\u001b[m Modified prepare.ipynb and generated output\n",
      "\u001b[33mf2a81a0\u001b[m Downloaded Data and Segmented\n",
      "\u001b[33meb4aabb\u001b[m Created prepare and train notebooks\n",
      "\u001b[33me9305a1\u001b[m Initial commit\n"
     ]
    }
   ],
   "source": [
    "!git log --oneline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
